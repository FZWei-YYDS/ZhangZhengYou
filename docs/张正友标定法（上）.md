



> 参考链接：
>
> 1. [相机标定之张正友标定法数学原理详解（含python源码）](https://zhuanlan.zhihu.com/p/94244568)
> 2. [张正友标定法Python详细实现](https://github.com/goldbema/CameraCalibration)
> 3. [单应矩阵的作用](https://zhuanlan.zhihu.com/p/427190988)
> 4. [线性方程组-直接法 2：Cholesky分解](https://zhuanlan.zhihu.com/p/387603571)

# 01 线性方程求解

线性方程组的矩阵形式：
$$
A = \left[ {\begin{array}{*{20}{c}}
{{a_{11}}}&{{a_{12}}}& \cdots &{{a_{1n}}}\\
{{a_{21}}}&{{a_{22}}}& \cdots &{{a_{2n}}}\\
 \vdots & \vdots & \ddots & \vdots \\
{{a_{m1}}}&{{a_{m2}}}& \cdots &{{a_{mn}}}
\end{array}} \right],x = \left[ {\begin{array}{*{20}{c}}
{{x_1}}\\
{{x_2}}\\
 \vdots \\
{{x_n}}
\end{array}} \right],b = \left[ {\begin{array}{*{20}{c}}
{{b_1}}\\
{{b_2}}\\
 \vdots \\
{{b_m}}
\end{array}} \right]
 \to
A x = b
$$
如果 $A$ 为方阵，且满秩，则方程有唯一的解 $x = A^{-1} b$。但一般来说，矩阵求逆的运算过于昂贵，我们根据系数矩阵，将实际的问题归为两类：

1. 系数矩阵 $A$ 稠密，规模不大，右端项 $b$ 一直变化：此类问题主要关注如何避免重复计算。 常用的是迭代法：Gauss消去法、LU分解、Cholesky分解法。
2. 系数矩阵 $A$ 稀疏，规模很大，直接求解代价很大：常用的是：$Jacobi$ 迭代法、Gauss-Seidel迭代法、超松弛迭代法。

## 1.1 Gauss消元法

考虑下述方程：
$$
Ax = b \to
\left\{ {\begin{array}{*{20}{c}}
{2{x_1} + {x_2} - {x_3} = 8}\\
{ - 3{x_1} - {x_2} + 2{x_3} =  - 11}\\
{ - 2{x_1} + {x_2} + 2{x_3} =  - 3}
\end{array}} \right.
$$
将 $A$ 和 $b$ 写成增广矩阵：
$$
\left[ {\left. {\begin{array}{*{20}{c}}
2&1&{ - 1}\\
{ - 3}&{ - 1}&2\\
{ - 2}&1&2
\end{array}} \right|\begin{array}{*{20}{c}}
8\\
{ - 11}\\
{ - 3}
\end{array}} \right]
$$
进行初等行变换，即Gauss消元法，可得：
$$
\begin{array}{l}
\left[ {\begin{array}{*{20}{c}}
2&1&{ - 1}\\
{ - 3}&{ - 1}&2\\
{ - 2}&1&2
\end{array}\left| {\begin{array}{*{20}{c}}
8\\
{ - 11}\\
{ - 3}
\end{array}} \right.} \right]\mathop  \to \limits^{{E_1}:\left\{ {\begin{array}{*{20}{c}}
{{r_2} = {r_2} + \frac{3}{2}{r_1}}\\
{{r_3} = {r_3} + {r_1}}
\end{array}} \right.} \left[ {\begin{array}{*{20}{c}}
2&1&{ - 1}\\
0&{1/2}&{1/2}\\
0&2&1
\end{array}\left| {\begin{array}{*{20}{c}}
8\\
1\\
5
\end{array}} \right.} \right]\mathop  \to \limits^{{E_2}:\left\{ {{r_2} = 2{r_2}} \right.} \left[ {\begin{array}{*{20}{c}}
2&1&{ - 1}\\
0&1&1\\
0&2&1
\end{array}\left| {\begin{array}{*{20}{c}}
8\\
2\\
5
\end{array}} \right.} \right]\\
\mathop  \to \limits^{{E_3}:\left\{ {{r_3} = {r_3} - 2{r_2}} \right.} \left[ {\begin{array}{*{20}{c}}
2&1&{ - 1}\\
0&1&1\\
0&0&{ - 1}
\end{array}\left| {\begin{array}{*{20}{c}}
8\\
2\\
1
\end{array}} \right.} \right]\mathop  \to \limits^{{E_4}:\left\{ {{r_3} =  - {r_3}} \right.} \left[ {\begin{array}{*{20}{c}}
2&1&{ - 1}\\
0&1&1\\
0&0&1
\end{array}\left| {\begin{array}{*{20}{c}}
8\\
2\\
{ - 1}
\end{array}} \right.} \right]\mathop  \to \limits^{{E_5}:\left\{ {\begin{array}{*{20}{c}}
{{r_2} = {r_2} - {r_3}}\\
{{r_1} = {r_1} + {r_3}}
\end{array}} \right.} \left[ {\begin{array}{*{20}{c}}
2&1&0\\
0&1&0\\
0&0&1
\end{array}\left| {\begin{array}{*{20}{c}}
7\\
3\\
{ - 1}
\end{array}} \right.} \right]\\
\mathop  \to \limits^{{E_6}:\left\{ {{r_1} = {r_1} - {r_2}} \right.} \left[ {\begin{array}{*{20}{c}}
2&0&0\\
0&1&0\\
0&0&1
\end{array}\left| {\begin{array}{*{20}{c}}
4\\
3\\
{ - 1}
\end{array}} \right.} \right]\mathop  \to \limits^{{E_7}:\left\{ {{r_1} = {r_1}/2} \right.} \left[ {\begin{array}{*{20}{c}}
1&0&0\\
0&1&0\\
0&0&1
\end{array}\left| {\begin{array}{*{20}{c}}
2\\
3\\
{ - 1}
\end{array}} \right.} \right]
\end{array}
$$
最终方程的解：
$$
x=[2, 3, -1] ^T
$$
注：在实际实现的时候，由于计算精度问题，通常我们调整主元最大的放前面进行消除，避免计算误差。

## 1.2 LU分解

如果仅将系数矩阵的通过初等行变换变成行阶梯矩阵：
$$
\left[ {\begin{array}{*{20}{c}}
2&1&{ - 1}\\
{ - 3}&{ - 1}&2\\
{ - 2}&1&2
\end{array}} \right]\mathop  \to \limits^{{E_1}:\left\{ {\begin{array}{*{20}{c}}
{{r_2} = {r_2} + \frac{3}{2}{r_1}}\\
{{r_3} = {r_3} + {r_1}}
\end{array}} \right.} \left[ {\begin{array}{*{20}{c}}
2&1&{ - 1}\\
0&{1/2}&{1/2}\\
0&2&1
\end{array}} \right]\mathop  \to \limits^{{E_2}:\left\{ {{r_3} = {r_3} - 4{r_2}} \right.} \left[ {\begin{array}{*{20}{c}}
2&1&{ - 1}\\
0&{1/2}&{1/2}\\
0&0&{ - 1}
\end{array}} \right]
$$
其中：
$$
{E_1} = \left[ {\begin{array}{*{20}{c}}
1&0&0\\
{3/2}&1&0\\
1&0&1
\end{array}} \right],{E_2} = \left[ {\begin{array}{*{20}{c}}
1&0&0\\
0&1&0\\
0&{ - 4}&1
\end{array}} \right]
$$
将最后的最简行阶梯矩阵记为：
$$
U = 
\left[ {\begin{array}{*{20}{c}}
2&1&{ - 1}\\
0&{1/2}&{1/2}\\
0&0&{ - 1}
\end{array}} \right]
$$
显然有：
$$
EA=E_2E_1A = U
$$
那么反过来：
$$
A=E^{-1}U = LU
$$
经过计算:
$$
L=E^{-1} =(E_2E_1)^{-1}=

\left[ {\begin{array}{*{20}{c}}
1       &0   &0\\
{- 3/2} &{1} &0\\
{- 1}   &4   &{1}
\end{array}} \right]
$$
于是之前的方程就可以分解为：
$$
\begin{array}{*{20}{c}}
{\underbrace {\left[ {\begin{array}{*{20}{c}}
2&1&{ - 1}\\
{ - 3}&{ - 1}&2\\
{ - 2}&1&2
\end{array}} \right]}_Ax = \underbrace {\left[ {\begin{array}{*{20}{c}}
8\\
{ - 11}\\
{ - 3}
\end{array}} \right]}_b}\\
 \downarrow \\
{\underbrace {\left[ {\begin{array}{*{20}{c}}
1&0&0\\
{ - 3/2}&1&0\\
{ - 1}&4&1
\end{array}} \right]}_L\underbrace {\left[ {\begin{array}{*{20}{c}}
2&1&{ - 1}\\
0&{1/2}&{1/2}\\
0&0&1
\end{array}} \right]}_Ux = \underbrace {\left[ {\begin{array}{*{20}{c}}
8\\
{ - 11}\\
{ - 3}
\end{array}} \right]}_b}
\end{array}
$$
该方法称为LU分解法：
$$
Ax = b \to L  ({Ux})  = b \Leftrightarrow \left\{ {\begin{array}{*{20}{c}}
{Ly = b}\\
{Ux = y}
\end{array}} \right.
$$
其中：$L$ 为下三角矩阵，$U$ 为上三角矩阵。

> 这样做的好处是什么呢？我们来考量 $Ax=b$ 和 $LUx=b$ 两个方程的时间复杂度，假设 $A$ 的规模为 $n \times n$，且满秩，考虑最复杂情况（仅考虑乘法运算，加法的运算在计算机中时间消耗可以忽略不计）：
>
> - **Gauss消元法**：第一次消去需要 $n ^ 2$ 次乘法运算（有 $n$ 行，每一行有$n$个元素，因此需要 $n$ 次乘法），第二次为 $(n+1)^2$，因此至少：
>   $$
>   {n^2} + {\left( {n - 1} \right)^2} +  \cdots  + 1 \approx \frac{1}{3}{n^3} = o\left( {{n^3}} \right)
>   $$
>
> - **LU分解**：如果系数矩阵为三角形，最后一行的运算为 1 次乘法，倒数第二行 2 次，因此最终：
>   $$
>   1 + 2 +  \cdots  + n = o\left( {{n^2}} \right)
>   $$
>
> 相比较而言，后者的算法复杂度仅为 $o(n^2)$，因此可以大幅提高计算效率。而之前 $A=LU$ 的分解，因为只需要分解一次，跟 $b$ 不相关，因此分解的计算量也可以忽略不计。

## 1.2 Cholesky分解

> 设 $A \in {\mathbb{R}_{n \times n}}$，若 $A=A^T$，对任意的 $X \in {\mathbb{R}_{n \times n}}$ 都有：$X^TAX>0$，则称 $A$ 为正定矩阵。

对于某些特殊形状的矩阵，例如对称正定矩阵：
$$
A=	\left[ {\begin{array}{*{20}{c}}
{36}&{48}&6\\
{48}&{68}&{10}\\
6&{10}&3
\end{array}} \right]
$$

显然我们有一些方法可以降低上述分解的计算量，这就是Cholesky分解！回顾之前的分解：
$$
{\underbrace {\left[ {\begin{array}{*{20}{c}}
1&0&0\\
{ - 3/2}&1&0\\
{ - 1}&4&1
\end{array}} \right]}_L\underbrace {\left[ {\begin{array}{*{20}{c}}
2&1&{ - 1}\\
0&{1/2}&{1/2}\\
0&0&1
\end{array}} \right]}_Ux = \underbrace {\left[ {\begin{array}{*{20}{c}}
8\\
{ - 11}\\
{ - 3}
\end{array}} \right]}_b}
$$
可以发现：

- $L$ 的对角元素都为1，因为 $L$ 仅描述了消元作用，不会改变主元。这类对角元素都为1的三角矩阵，我们称为单位三角阵。
- $U$ 的对角元素是任意的。

如果把 $U$ 进一步分解为对角阵 $D$ 和单位上三角矩阵 $U_0$：
$$
U = D{U_0}
$$
例如（通过列方程）：
$$
\begin{array}{*{20}{c}}
{\underbrace {\left[ {\begin{array}{*{20}{c}}
6&8&1\\
0&2&1\\
0&0&1
\end{array}} \right]}_U = \underbrace {\left[ {\begin{array}{*{20}{c}}
6&0&0\\
0&2&0\\
0&0&1
\end{array}} \right]}_D\underbrace {\left[ {\begin{array}{*{20}{c}}
1&{{x_1}}&{{x_2}}\\
0&1&{{x_3}}\\
0&0&1
\end{array}} \right]}_{{U_0}}}\\
 \downarrow \\
{\begin{array}{*{20}{c}}
{8 = 6{x_1}}\\
{1 = 6{x_2}}\\
{1 = 2{x_3}}
\end{array} \to \begin{array}{*{20}{c}}
{{x_1} = 4/3}\\
{{x_2} = 1/6}\\
{{x_3} = 1/2}
\end{array}}
\end{array}
$$
此时对 $A$ 的分解变为：
$$
A=LDU_0
$$
对于对称矩阵来说，$L=U_0^T$，证明如下，由于$A=A^T$：
$$
\begin{array}{C}
A = LU = LD{U_0}\\
{A^T} = {\left( {LD{U_0}} \right)^T} = U_0^T{\left( {LD} \right)^T} = U_0^T{D^T}{L^T}= U_0^T{D}{L^T}  
\end{array}
$$
由于针对满秩矩阵，$A=LU$ 和后续的$U=DU_0$两次分解都具有唯一性，因此：$L=U_0^T$。因此最终：
$$
A=LDL^T
$$
将公式反过来：
$$
D=L^{-1}A(L^T)^{-1} = L^{-1}A(L^{-1})^T
$$
令 $(L^{-1})^T=X$，上式可以写作：
$$
D=X^TAX
$$
由于 $A$ 正定矩阵（所谓正定矩阵，可以看上面定义），因此 $D$ 恒大于 0。因此 $D$ 可以被开根号：
$$
D = \sqrt{D} \sqrt{D}
$$
因此最终 $A$ 可以写为：
$$
A= (L \sqrt{D})(\sqrt{D}L ^T) = \widetilde L{\widetilde L^T}
$$
可以发现：对称正定阵可以被分解为一个下三角阵 $\widetilde{L}$ 及其转置的乘积。

---

> 对于对称正定矩阵，我们只需要得到一个下三角矩阵，就可以将矩阵分解，显然效率高了一些。下面我们来讨论具体如何进行分解。

假设对称正定矩阵 $A$ 可以经过Cholesky分解得到：
$$
A= (L \sqrt{D})(\sqrt{D}L ^T) = \widetilde L{\widetilde L^T}
$$

> 分块矩阵记号：
>
> ![image-20220524214056187](https://flyman-cjb.oss-cn-hangzhou.aliyuncs.com/picgos/image-20220524214056187.png)

将 $A=\widetilde L{\widetilde L^T}$ 进行分块：
$$
\left[ {\begin{array}{*{20}{c}}
  {{a_{11}}}&{A_{21}^T} \\ 
  {{A_{21}}}&{{A_{22}}} 
\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
  {{l_{11}}}&{} \\ 
  {{L_{21}}}&{{L_{22}}} 
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
  {{l_{11}}}&{L_{21}^T} \\ 
  {}&{L_{22}^T} 
\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
  {l_{11}^2}&{{l_{11}}L_{21}^T} \\ 
  {{l_{11}}{L_{21}}}&{{L_{21}}L_{21}^T + {L_{22}}L_{22}^T} 
\end{array}} \right]
$$
那么根据公式，$L$ 的求解步骤如下：

1. 求解：$l_{11}, L_{21}$
   $$
   \begin{array}{l}
     {l_{11}} &= \sqrt {{a_{11}}}  \hfill \\
     {L_{21}} &= \frac{1}{{{l_{11}}}}{A_{21}} \hfill \\ 
   \end{array}
   $$

2. 求解：$L_{22}$
   $$
   {L_{22}}L_{22}^T = {A_{22}} - {L_{21}}L_{21}^T
   $$

3. 递归求解，直到 $L_{22}$ 为单个数，而非矩阵。

整个递归过程类似于：

![image-20220524212337749](https://flyman-cjb.oss-cn-hangzhou.aliyuncs.com/picgos/image-20220524212337749.png)

由于Cholesky分解相比较LU分解不需要求逆，因此在大规模问题上，Cholesky分解的速度可以快很多。

---

以如下矩阵为例进行Cholesky分解：
$$
A = \left[ {\begin{array}{*{20}{c}}
  4&{12}&{ - 16} \\ 
  {12}&{37}&{ - 43} \\ 
  { - 16}&{ - 43}&3 
\end{array}} \right]
$$
Step 1:
$$
\begin{gathered}
  \left[ {\begin{array}{*{20}{c}}
  4&\vline & {12}&{ - 16} \\ 
\hline
  {12}&\vline & {37}&{ - 43} \\ 
  { - 16}&\vline & { - 43}&{98} 
\end{array}} \right] \hfill \\
   \downarrow  \hfill \\
  \begin{array}{*{20}{c}}
  {{l_{11}} = \sqrt {{a_{11}}}  = \sqrt 4  = 2} \\ 
  {{L_{21}} = \frac{1}{{{l_{11}}}}{A_{21}} = \frac{1}{2}\left[ {\begin{array}{*{20}{c}}
  {12} \\ 
  { - 16} 
\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
  6 \\ 
  { - 8} 
\end{array}} \right]} \\ 
  {{L_{22}}L_{22}^T = {A_{22}} - {L_{21}}L_{21}^T = \left[ {\begin{array}{*{20}{c}}
  {37}&{ - 43} \\ 
  { - 43}&{98} 
\end{array}} \right] - \left[ {\begin{array}{*{20}{c}}
  6 \\ 
  { - 8} 
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
  6&{ - 8} 
\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
  1&5 \\ 
  5&{34} 
\end{array}} \right]} 
\end{array} \hfill \\ 
\end{gathered}
$$
Step 2：
$$
\begin{gathered}
  \left[ {\begin{array}{*{20}{c}}
  1&5 \\ 
  5&{34} 
\end{array}} \right] \hfill \\
   \downarrow  \hfill \\
  \begin{array}{*{20}{c}}
  {{l_{11}} = \sqrt {{a_{11}}}  = \sqrt 1  = 1} \\ 
  {{L_{21}} = \frac{1}{{{l_{11}}}}{A_{21}} = \frac{1}{1}\left[ 5 \right] = \left[ 5 \right]} \\ 
  {{L_{22}}L_{22}^T = {A_{22}} - {L_{21}}L_{21}^T = \left[ {34} \right] - \left[ 5 \right]{{\left[ 5 \right]}^T} = 9} 
\end{array} \hfill \\ 
\end{gathered}
$$
Step 3:
$$
l_{11} = \sqrt{a_{11}} = \sqrt{9}=3
$$
最后求解的矩阵仅为一维，直接开根号即可。

依次填入：
$$
\widetilde L = \left[ {\begin{array}{*{20}{c}}
   \times &0&0 \\ 
   \times & \times &0 \\ 
   \times & \times & \times  
\end{array}} \right] \to \left[ {\begin{array}{*{20}{c}}
   \times &0&0 \\ 
   \times & \times &0 \\ 
   \times & \times &3 
\end{array}} \right] \to \left[ {\begin{array}{*{20}{c}}
   \times &0&0 \\ 
   \times &1&0 \\ 
   \times &5&3 
\end{array}} \right] \to \left[ {\begin{array}{*{20}{c}}
  2&0&0 \\ 
  6&1&0 \\ 
  { - 8}&5&3 
\end{array}} \right]
$$
于是最终：
$$
A = \widetilde L{\widetilde L^T} = \left[ {\begin{array}{*{20}{c}}
  2&0&0 \\ 
  6&1&0 \\ 
  { - 8}&5&3 
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
  2&6&{ - 8} \\ 
  0&1&5 \\ 
  0&0&3 
\end{array}} \right] = A = \left[ {\begin{array}{*{20}{c}}
  4&{12}&{ - 16} \\ 
  {12}&{37}&{ - 43} \\ 
  { - 16}&{ - 43}&3 
\end{array}} \right]
$$


# 02 最小二乘问题-线性方法

## 2.1 特征值分解

设 $A$ 为 $n$ 阶方阵，若存在数 $\lambda$ 和非零向量 $v$，使得：
$$
A v = \lambda v (v \ne 0)
$$
则称：

- $\lambda$ ：矩阵 $A$ 的一个**特征值**
- $v$ ：矩阵 $A$ 的对应于特征值 $\lambda$ 的**特征向量**。

> **补充1：特征值、特征向量求解**
>
> 例：求解三阶实矩阵 $A = \left[ {\begin{array}{*{20}{c}}
>   3&2&4 \\ 
>   2&0&2 \\ 
>   4&2&3 
> \end{array}} \right]$ 的所有特征值和特征向量。
>
> - Step 1：计算 $A$ 的特征多项式
>   $$
>   f\left( \lambda  \right) = \left| {\lambda E - A} \right| = \left| {\begin{array}{*{20}{c}}
>     {\lambda  - 3}&{ - 2}&{ - 4} \\ 
>     { - 2}&\lambda &{ - 2} \\ 
>     { - 4}&{ - 2}&{\lambda  - 3} 
>   \end{array}} \right| = \left( {\lambda  - 8} \right){\left( {\lambda  + 1} \right)^2}
>   $$
>   因此特征值：$\lambda_1 = 8, \lambda_2 = \lambda_3 = -1$。
>
> - Step 2: 求解特征值相应线性方程
>   $$
>   \begin{gathered}
>     \left( {{\lambda _1}E - A} \right)x = 0 \hfill \\
>      \downarrow  \hfill \\
>      = \left| {\begin{array}{*{20}{c}}
>     5&{ - 2}&{ - 4} \\ 
>     { - 2}&8&{ - 2} \\ 
>     { - 4}&{ - 2}&5 
>   \end{array}} \right| \to \left| {\begin{array}{*{20}{c}}
>     1&{ - 2/5}&{ - 4/5} \\ 
>     { - 2}&8&{ - 2} \\ 
>     0&{ - 18}&9 
>   \end{array}} \right| \to \left| {\begin{array}{*{20}{c}}
>     1&{ - 2/5}&{ - 4/5} \\ 
>     0&{\frac{{36}}{5}}&{ - \frac{{18}}{5}} \\ 
>     0&2&{ - 1} 
>   \end{array}} \right| \hfill \\
>      \to \left| {\begin{array}{*{20}{c}}
>     1&{ - 2/5}&{ - 4/5} \\ 
>     0&2&{ - 1} \\ 
>     0&0&0 
>   \end{array}} \right| \to \left| {\begin{array}{*{20}{c}}
>     1&0&{ - 1} \\ 
>     0&1&{ - 1/2} \\ 
>     0&0&0 
>   \end{array}} \right| \hfill \\ 
>   \end{gathered}
>   $$
>
>   $$
>   \begin{gathered}
>     \left( {A - {\lambda _2}E} \right)x = 0 \hfill \\
>      \downarrow  \hfill \\
>      = \left| {\begin{array}{*{20}{c}}
>     4&2&4 \\ 
>     2&1&2 \\ 
>     4&2&4 
>   \end{array}} \right| \to \left| {\begin{array}{*{20}{c}}
>     1&{1/2}&1 \\ 
>     0&0&0 \\ 
>     0&0&0 
>   \end{array}} \right| \hfill \\ 
>   \end{gathered}
>   $$
>
>   特征向量（基础解析）：
>   $$
>   {x_1} = \left[ {\begin{array}{*{20}{c}}
>     1 \\ 
>     {1/2} \\ 
>     1 
>   \end{array}} \right],{x_2} = \left[ {\begin{array}{*{20}{c}}
>     { - 1/2} \\ 
>     1 \\ 
>     0 
>   \end{array}} \right],{x_3} = \left[ {\begin{array}{*{20}{c}}
>     { - 1} \\ 
>     0 \\ 
>     1 
>   \end{array}} \right]
>   $$

矩阵 $A$ 即可以被分解为下列形式：
$$
A = Q \sum Q^{-1}
$$
其中：

- $Q$：该矩阵特征向量组成的矩阵
- $\sum$：特征值做成的对角矩阵

> 补充2：矩阵分解证明
> $$
> \begin{gathered}
>   AQ = A\left[ {\begin{array}{*{20}{c}}
>   {{x_1}}&{{x_2}}& \cdots &{{x_n}} 
> \end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
>   {A{x_1}}&{A{x_2}}& \cdots &{A{x_n}} 
> \end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
>   {{\lambda _1}{x_1}}&{{\lambda _2}{x_2}}& \cdots &{{\lambda _n}{x_n}} 
> \end{array}} \right] \hfill \\
>   Q\sum {}  = \left[ {\begin{array}{*{20}{c}}
>   {{x_1}}&{{x_2}}& \cdots &{{x_n}} 
> \end{array}} \right]\left[ {\begin{array}{*{20}{c}}
>   {{\lambda _1}}&0& \cdots &0 \\ 
>   0&{{\lambda _2}}& \cdots &0 \\ 
>    \vdots &{}& \ddots & \vdots  \\ 
>   0&0& \cdots &{{\lambda _n}} 
> \end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
>   {{\lambda _1}{x_1}}&{{\lambda _2}{x_2}}& \cdots &{{\lambda _n}{x_n}} 
> \end{array}} \right] \hfill \\
>    \downarrow  \hfill \\
>   AQ = Q\sum {}  \to A = Q\sum {{Q^{ - 1}}}  \hfill \\ 
> \end{gathered}
> $$

> **补充3：斯密特正交化**（利用线性无关向量组，构造出一个标准正交向量组）
>
> 设 $\alpha_1, \alpha_2, ..., \alpha_m (m \le n)$ 是 $\mathbb{R} ^n$ 的一个线性无关向量组，若令：
> $$
> \begin{gathered}
>   {\beta _1} = {\alpha _1} \hfill \\
>   {\beta _2} = {\alpha _2} - \frac{{\left\langle {{\alpha _2},{\beta _1}} \right\rangle }}{{\left\langle {{\beta _1},{\beta _1}} \right\rangle }}{\beta _1} \hfill \\
>   {\beta _m} = {\alpha _m} - \frac{{\left\langle {{\alpha _m},{\beta _1}} \right\rangle }}{{\left\langle {{\beta _1},{\beta _1}} \right\rangle }}{\beta _1} - \frac{{\left\langle {{\alpha _m},{\beta _2}} \right\rangle }}{{\left\langle {{\beta _2},{\beta _2}} \right\rangle }}{\beta _2} -  \cdots \frac{{\left\langle {{\alpha _m},{\beta _{m - 1}}} \right\rangle }}{{\left\langle {{\beta _{m - 1}},{\beta _{m - 1}}} \right\rangle }}{\beta _{m - 1}} \hfill \\ 
> \end{gathered}
> $$
> 则 $\beta _1, \beta_2,...,\beta_m$ 就是一个正交向量组，若令：
> $$
> {e_i} = \frac{{{\beta _i}}}{{\left\| {{\beta _i}} \right\|}}\left( {i = 1,2,...,m} \right)
> $$
> 则得到一个标准正交向量组：$e_1,e_2,....,e_m$，则该向量组与 $\alpha_1, \alpha_2,... \alpha_m$ 等价。
>
> 注：$<a,b>$ 代表两个向量之间的夹角，计算公式如下：$\frac{a \cdot b}{||a|| \cdot ||b||}$ （(ab的内积)/(|a||b|)）。

假如对特征向量进行施密特正交化，即可得到标准正交向量组，它们变成了正交矩阵，即有：
$$
A = Q \sum Q^T
$$

> 补充4：正交矩阵
>
> 若 $A$ 为 $n$ 阶实矩阵，如果 $A$ 满足：
> $$
> AA^T = A^TA = E
> $$
> 则称 $A$ 为正交矩阵，有以下性质：
>
> 1. $A^{-1} =A^T$
>
> 2. $|A|=  \pm 1$
>
> 3. $A^T,A^{-1}$ 仍然是正交矩阵
>
> 4. $A,B$ 为正交矩阵，则 $AB$ 依然为正交矩阵
>
> 5. 保范性：正交矩阵对向量进行正交变换，且正交变换不改变向量的长度(范数)
>    $$
>    (Ax)^T(Ax)=x^TA^TAx=x^Tx
>    $$

那么这么做的好处是什么呢？假如一个物理系统可以用一个矩阵表示（这在实际物理系统里非常常见，例如谐波振荡器），那么这个物理系统的特性就是被这个矩阵的特征值所决定的，各种不同的信号（向量）进入这个系统，系统输出的信号就会发生变化，只有在输入信号靠近矩阵的特征向量，信号才会被稳定放大。

## 2.2 奇异值分解

> 前面的要求是矩阵必须是 $n \times n$ 的方阵，那么如果不是方阵呢？那么就是SVD奇异值分解。

假设矩阵 $A$ 是一个 $m \times n $ 的矩阵，那么定义矩阵 $A$ 的SVD为：
$$
A = U \sum V^T
$$
其中：

- $U$：$m \times m$ 矩阵，酉矩阵，$U^TU=I$
- $\sum$：$m \times n$ 矩阵（除了主对角线上的元素以外全为0），主对角线上的每个元素都称为奇异值
- $V$：$n\times n$ 矩阵，酉矩阵，$V^TV=I$

> 补充5：奇异值计算
>
> **左奇异值、矩阵 $U$：**
> $$
> \left( {A{A^T}} \right){u_i} = {\lambda _i}{u_i}
> $$
> **右奇异值、矩阵：**
> $$
> \left( {{A^T}A} \right){v_i} = {\lambda _i}{v_i}
> $$
> 由于 $\sum$ 除了对角线上是奇异值，其他位置都是0，我们注意到：
> $$
> \begin{array}{c}
> A = U\sum {{V^T}}  \to AV = U\sum {{V^T}} V \to AV = U\sum {} \\
>   \downarrow \\
> A{v_i} = {\sigma _i}{u_i} \\ \downarrow \\ {\sigma _i} = A{v_i}/{u_i}
> \end{array}
> $$
> 求出奇异值矩阵 $\sum$ 。也可以用过：$\sigma_i=\sqrt{\lambda_i}$ 来计算奇异值。奇异值的数量与 $r$ 矩阵的秩相关。
>
> ---
>
> 例：矩阵 $A$ 定义为：
> $$
> A = \left[ {\begin{array}{*{20}{c}}
> 0&1\\
> 1&1\\
> 1&0
> \end{array}} \right]
> $$
> 首先求出：$A^TA$和 $A A^T$：
> $$
> \begin{array}{l}
> {A^T}A = \left[ {\begin{array}{*{20}{c}}
> 0&1&1\\
> 1&1&0
> \end{array}} \right]\left[ {\begin{array}{*{20}{c}}
> 0&1\\
> 1&1\\
> 1&0
> \end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
> 2&1\\
> 1&2
> \end{array}} \right]\\
> A{A^T} = \left[ {\begin{array}{*{20}{c}}
> 0&1\\
> 1&1\\
> 1&0
> \end{array}} \right]\left[ {\begin{array}{*{20}{c}}
> 0&1&1\\
> 1&1&0
> \end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
> 1&1&0\\
> 1&2&1\\
> 0&1&1
> \end{array}} \right]
> \end{array}
> $$
> 进而求出 $A^T A$ 的特征值和特征向量（根据特征值从大到小排列）：
> $$
> \begin{array}{l}
> {\lambda _1} = 3,{v_1} = \left[ {\begin{array}{*{20}{c}}
> {1/\sqrt 2 }\\
> {1/\sqrt 2 }
> \end{array}} \right], \quad 
> {\lambda _2} = 1,{v_2} = \left[ {\begin{array}{*{20}{c}}
> { - 1/\sqrt 2 }\\
> {1/\sqrt 2 }
> \end{array}} \right]
> \end{array}
> $$
> 接着求出 $AA^T$ 的特征值和特征向量：
> $$
> {\lambda _1} = 3,{u_1} = \left[ {\begin{array}{*{20}{c}}
> {1/\sqrt 6 }\\
> {2/\sqrt 6 }\\
> {1/\sqrt 6 }
> \end{array}} \right], \quad
> 
> {\lambda _2} = 1,{u_2} = \left[ {\begin{array}{*{20}{c}}
> {1/\sqrt 2 }\\
> 0\\
> { - 1/\sqrt 2 }
> \end{array}} \right], \quad
> {\lambda _3} = 0,{u_3} = \left[ {\begin{array}{*{20}{c}}
> {1/\sqrt 3 }\\
> { - 1/\sqrt 3 }\\
> {1/\sqrt 3 }
> \end{array}} \right]
> $$
> 利用 $A v_i = \sigma _ i u_i$：
> $$
> \begin{array}{}
> \left[ {\begin{array}{*{20}{c}}
> 0&1\\
> 1&1\\
> 1&0
> \end{array}} \right]\left[ {\begin{array}{*{20}{c}}
> {1/\sqrt 2 }\\
> {1/\sqrt 2 }
> \end{array}} \right] = {\sigma _1}\left[ {\begin{array}{*{20}{c}}
> {1/\sqrt 6 }\\
> {2/\sqrt 6 }\\
> {1/\sqrt 6 }
> \end{array}} \right] \to {\sigma _1} = \sqrt 3 \\
> \left[ {\begin{array}{*{20}{c}}
> 0&1\\
> 1&1\\
> 1&0
> \end{array}} \right]\left[ {\begin{array}{*{20}{c}}
> { - 1/\sqrt 2 }\\
> {1/\sqrt 2 }
> \end{array}} \right] = {\sigma _2}\left[ {\begin{array}{*{20}{c}}
> {1/\sqrt 2 }\\
> 0\\
> { - 1/\sqrt 2 }
> \end{array}} \right] \to {\sigma _2} = 1
> \end{array}
> $$
> 可以验证：直接利用：$\sigma_i = \sqrt{ \lambda _ i}$ 进行求解，值是一样的。于是最终奇异值分解：
> $$
> A = U\sum {{V^T}}  = \left[ {\begin{array}{*{20}{c}}
> {1/\sqrt 6 }&{1/\sqrt 2 }&{1/\sqrt 3 }\\
> {2/\sqrt 6 }&0&{ - 1/\sqrt 3 }\\
> {1/\sqrt 6 }&{ - 1/\sqrt 2 }&{1/\sqrt 3 }
> \end{array}} \right]\left[ {\begin{array}{*{20}{c}}
> {\sqrt 3 }&0\\
> 0&1\\
> 0&0
> \end{array}} \right]\left[ {\begin{array}{*{20}{c}}
> {1/\sqrt 2 }&{1/\sqrt 2 }\\
> { - 1/\sqrt 2 }&{1/\sqrt 2 }
> \end{array}} \right]
> $$
>

奇异值跟我们之前特征分解中的特征值相类似，将奇异值从大到小排列，很多情况下，前 $10 {\text{\% }}$甚至 $1 {\text{\% }}$的奇异值就占了全部奇异值之和的 $99 \text{\%}$ 以上，即：
$$
{A_{m \times n}} = {U_{m \times m}}\sum\nolimits_{m \times n} {V_{n \times n}^T}  \approx {U_{m \times k}}\sum\nolimits_{k \times k} {V_{k \times n}^T}
$$
其中：$k$ 远远小于 $n$。

## 2.3 超定线性方程

考虑线性方程：
$$
Ax=b \\
A \in {\mathbb{R}_{m \times n}},x \in {\mathbb{R}_{n \times 1}},b \in {\mathbb{R}_{m \times 1}}
$$
$m$ 个方程求解 $n$ 个未知数，有三种情况：

- $m=n$，且 $A$ 为满秩矩阵，则有唯一解：$x= A^{-1} b$
- $m < n$，欠定问题，无数解（可以看相应教材）
- $m > n$，约束的个数大于未知数个数，称为超定问题

通常我们遇到的都是超定问题，此时 $Ax = b$ 不存在解，转而求最小二乘问题：
$$
J(x)=\min \left\| {Ax - b} \right\|_2^2 \\
$$
$J(x)$ 是[凸函数](https://blog.csdn.net/jgj123321/article/details/105945705/)（二阶导数非负），我们令一阶导数为0，可以得到：
$$
A^TAx-A^Tb=0
$$
进而方程的解：
$$
x=(A^TA)^{-1}A^Tb
$$

其中：$(A^TA)^{-1}A^T$ 又称为违逆，因为它和方阵的 $x = A^{-1} b$ 是一样的

由于这里需要取逆操作，计算量较大，并且 $A^TA$ 还有可能存在“病态”，甚至不可逆的情况，因此实际情况更多的是用 SVD 方法来求解超定方程，也就是最小二乘问题。

## 2.4 最小二乘SVD法

针对**非齐次方程**，设 $A \in {\mathbb{R}_{m \times n}}$ 列满秩，而 $A = U\left[ {\begin{array}{*{20}{c}}
  {\sum {} } \\ 
  0 
\end{array}} \right]{V^T}$ 是 $A$  的奇异值分解，令：$U_n$ 为 $U$ 的前 $n$ 列矩阵，即：$U = \left[ {{U_n},\overline U } \right]$，则：
$$
min \left\| {Ax - b} \right\|_2^2 = \left\| {U\left[ {\begin{array}{*{20}{c}}
  {\sum {} } \\ 
  0 
\end{array}} \right]{V^T}x - b} \right\|_2^2
$$
由于正交矩阵的保范性，同乘以 $U^T$，则有（范数实际是平方和，因此最后一步可以拆分）：
$$
\begin{array}{l}
 min \left\| {Ax - b} \right\|_2^2 
 &= \left\| {\left[ {\begin{array}{*{20}{c}}
  {\sum {} } \\ 
  0 
\end{array}} \right]{V^T}x - {U^T}b} \right\|_2^2 = \left\| {\left[ {\begin{array}{*{20}{c}}
  {\sum {} } \\ 
  0 
\end{array}} \right]{V^T}x - {{\left[ {{U_n},\overline U } \right]}^T}b} \right\|_2^2 \hfill \\
   &= \left\| {\left[ {\begin{array}{*{20}{c}}
  {\sum {{V^T}x - U_n^Tb} } \\ 
  { - {{\overline U }^T}b} 
\end{array}} \right]} \right\|_2^2 = \left\| {\sum {{V^T}x - U_n^Tb} } \right\|_2^2 + \left\| { - {{\overline U }^T}b} \right\|_2^2 \hfill \\ 
\end{array}
$$
其中：$\left\| { - {{\overline U }^T}b} \right\|_2^2$  不包含未知数，因此范数均$\ge 0$，因此当且仅当：$\sum {{V^T}x - U_n^Tb}  = 0$ 时成立，因此最终：
$$
x = {\left( {\sum {{V^T}} } \right)^{ - 1}}U_n^Tb = V{\sum {} ^{ - 1}}U_n^Tb
$$

---

针对**齐次方程**，即 $Ax=0$，推导如下：
$$
\min \left\| {Ax} \right\|_2^2 = \left\| {U\sum {{V^T}x} } \right\|_2^2 = \left\| {\sum {{V^T}x} } \right\|_2^2
$$
令：$z=V^Tx$，则上式变为：
$$
\begin{array}{l}
  \min \left\| {Ax} \right\|_2^2 
  &= \left\| {U\sum {{V^T}x} } \right\|_2^2 = \left\| {\sum {{V^T}x} } \right\|_2^2 \hfill \\
 &= \left\| {\sum z } \right\|_2^2 = {\left( {\sum z } \right)^T}\left( {\sum z } \right) = {z^T}{\sum {} ^2}z \hfill \\ 
\end{array}
$$
这是一个二次型，展开后可得：
$$
\min \left\| {Ax} \right\|_2^2 = \sigma _1^2z_1^2 + \sigma _2^2z_2^2 +  \cdots  + \sigma _n^2z_n^2 \quad 


{\sigma _1} \geqslant {\sigma _2} \geqslant  \cdots  \geqslant {\sigma _n} \geqslant 0
$$
约束 $||z||=1$（你也可以约束其他值，因为对于齐次方程来说无非是增加尺度系数），显然：$z =[0,0,...,1]^T$  是方程的最小解，因此：
$$
x=Vz=\left[ {\begin{array}{*{20}{c}}
  {{V_1}}&{{V_2}}& \cdots &{{V_n}} 
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
  0 \\ 
  0 \\ 
   \vdots  \\ 
  1 
\end{array}} \right] = {V_n}
$$
即 $A$ 的SVD分解右奇异矩阵 $V$ 的最后一列。

## 2.5 总体最小二乘法（TLS）

> 由于前面仅考虑了数据向量 $b$ 的误差，但实际情况是矩阵 $A$ 也存在误差，例如在张正友标定算法中，我们提取出来的角点位置信息肯定不是那么准确的，那么如何操作呢？这里时候用到总体最小二乘法来估计。

考虑矩阵方程：
$$
(A+E)x=b+e
$$
上式可以写作：
$$
\left( {\left[ { - b,A} \right] + \left[ { - e,E} \right]} \right)\left[ {\begin{array}{*{20}{c}}
  1 \\ 
  x 
\end{array}} \right] = 0
\\
\downarrow \\

\left( {B_{m \times (n + 1)} + D_{m \times (n + 1)}} \right)z_{(n +1)
\times 1} = 0 
\label{36}
$$
在这里，我们主要研究超定方程（$m \ge n$）情况，有两种可能的情况：

> 补充：求解二元函数在 $f(x,y)$ 在 $\varphi(x,y)=0$ 情况下的极值。
>
> 1. **化条件极值为无条件**：将 $y= \varphi(x)$ 显式表示出来，代入求一元函数的极值；
> 2. **拉格朗日函数法**：构造拉格朗日函数：$L(x,y, \lambda) = f(x,y) + \lambda \varphi(x,y)$，一阶偏导数为零

------

**1. ${\sigma _n}$ 明显比 $\sigma_ {n + 1}$ 大，即最小奇异值只有一个：**

将公式 $\ref{36}$ 改写为：
$$
Bz=-Dz=r
$$
我们当然希望误差 $Dz$ 带来的影响最小，如果我们约束 $z$ 是一个单位范数的向量，即 $z^Tz=1$（因为有无穷多解，所以等比缩放都可以）。那么最小二乘问题：
$$
\min \left\| {Dz} \right\|_2^2 \to \min \left\| {Bz} \right\|_2^2\quad st. z^Tz=1
$$
利用拉格朗日乘数法求解，令目标函数：
$$
J = \left\| {Bz} \right\|_2^2 + \lambda \left( {1 - {z^T}z} \right)=z^TB^TBz + \lambda(1-z^Tz)
$$
其中：$\lambda$ 为拉格朗日乘数，对 $z$ 求偏导，并平移：
$$
B^TBz = \lambda z
$$
这表明：

- 拉格朗日乘数 $\lambda$ ：应该选择矩阵：$B^TB$ 的最小特征值，即 $B$ 的最小奇异值的平方根（因为我们在求最小值）。
- 总体最小二乘解 $z$ ：最小奇异值对应的右奇异向量

令 $m \times (n + 1)$ 增广矩阵 $B$ 的奇异值：
$$
B = U \sum V ^T
$$
将奇异值按照：$\sigma_1 \ge \sigma_2 \ge ... \ge \sigma_{n + 1}$，特征向量也相应排列为：$v_1,v_2,...,v_{n+1}$，因此总体最小二乘解：$z=v_{n+1}。$将 $z^Tz=1$ 的约束去除，最终解：
$$
z = \left[ {\begin{array}{*{20}{c}}
  1 \\ 
  {{x_1}} \\ 
   \vdots  \\ 
  {{x_n}} 
\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
  {{v_{\left( {1,n + 1} \right)}}} \\ 
  {{v_{\left( {2,n + 1} \right)}}} \\ 
   \vdots  \\ 
  {{v_{\left( {n,n + 1} \right)}}} 
\end{array}} \right] \to {x_{TLS}} = \frac{1}{{{v_{\left( {1,n + 1} \right)}}}}\left[ {\begin{array}{*{20}{c}}
  {{v_{(2,n + 1)}}} \\ 
   \vdots  \\ 
  {{v_{\left( {n,n + 1} \right)}}} 
\end{array}} \right]
$$

---

**2. 奇异值多重（最后面若干个奇异值是重复的，非常接近的）**

不妨令：
$$
\sigma_1 \ge \sigma_2 \ ... \ge \sigma_p \ge \sigma_{p+1} \approx ... \approx \sigma_{n+1}
$$
且 $v_i$ 是子空间：
$$
S = Span\left\{ {{v_{p + 1}},{v_{p + 2}},...,{v_{n + 1}}} \right\}
$$
中的任一列向量，则上述任一右奇异向量 $v_i$ 都给出一组总体最小二乘解：
$$
x = \frac{y_i}{\alpha_i} \quad i=p+1,p+2,...,n+1
$$
其中：

- $\alpha_i$：向量 $v_i$ 的第一个元素
- $y_i$：向量 $v_i$ 的其他元素

因此：会有 $n+1-p$ 个总体最小二乘解。然而，我们可以找出两种意义的唯一解：**最小范数解**：解向量由 $n$ 个参数组成、**最优最小二乘近似解**：解向量仅包含 $p$ 个参数（详细的可以看相应的书籍）。

> 通常估计相机参数，我们用不到那么复杂的线性方法，通常只SVD方法获取初值，之后考虑重投影误差等因素，利用非线性优化整体优化相机内参、外参估计。

# 03 最小二乘问题-非线性优化

> 前面的最小二乘法都是线性的

考虑一个简单的最小二乘问题：
$$
\mathop {\min }\limits_x F\left( x \right) = \frac{1}{2}\left\| {f\left( x \right)} \right\|_2^2 \quad x \in {\mathbb{R}^n}
$$
假如 $f$ 是个数学形式上很简单的函数，那么完全可以采用解析的方式，目标函数的导数为零：
$$
\frac{{dF}}{{dx}} = 0
$$
然后就得到了极值点，将它们逐个比较，即可得知是极大、极小、还是鞍点。但是更多时候，导函数的形式比较复杂，我们很难直接求得导函数为0的点 ，因而难以得到初值。通常，我们先利用线性方法得到一个比较好的初值，然后利用迭代的方法，一步步寻找增量 $\Delta x_k$ 逐渐收敛到一个较好的值，也就是非线性优化。

## 3.1 最速下降法

考虑第 $k$ 次迭代，我们在 $x_k$ 附近进行泰勒展开：
$$
F\left( {{x_k} + \Delta {x_k}} \right) = F\left( {{x_k}} \right) + J{\left( {{x_k}} \right)^T}\Delta {x_k} + \frac{1}{2}\Delta x_k^TH\left( {{x_k}} \right)\Delta {x_k} + O\left( {\Delta {x_k}} \right)
$$
其中：

- $J(x_k)$：$F(x)$ 关于 $x$ 的一阶导数，即梯度，雅克比矩阵
- $H(x_k)$：二阶导数，海森矩阵

如果保留一阶梯度，取增量为反向的梯度，再指定步长 $\lambda$（有一定经验性），即可保证函数下降：
$$
\Delta x_k=-\lambda J(x_k)
$$
只要我们沿着反向梯度方向前进，那么目标函数一定会下降，但是这个方法只用到了一阶展开，如果选择的步长 $\lambda$ 过大，计算出的 $\Delta x_k$ 过大，这一近似就不够精确，并且过于贪心，很容易走出锯齿路线，太小的话迭代又很慢。

## 3.2 牛顿法

我们选择保留二阶梯度，并且对函数 $F(x)$ 关于 $\Delta x$ 求导，获得增量方程：
$$
J(x_k)+H(x_k) \Delta x_k=0 \to H(x_k) \Delta x_k = -J(x_k)
$$
当迭代求得的 $\Delta x_k$ 足够小时，迭代停止。之所以如此，有两个含义：

- 当导数等于 $0$ 时，方程 $F(x)$ 才会取到极值；
- 并且只有当 $\Delta x_k$ 足够小，该近似关系才足够准确；

该方法称为牛顿法，但是牛顿法需要计算目标函数的 $H$ 海森矩阵，计算量过大。

> 注：后续的推导中，为了表达方便，我们用 $x$ 替代每次的 $x_k$。

## 3.3 [高斯牛顿法](https://blog.csdn.net/weishaodong/article/details/107187743)

高斯牛顿法选择将 $f(x)$ 在 $x_K$ 附近进行一阶泰勒展开（这里不是$F(x)$，否则就成为了牛顿法），即：
$$
f\left( {x + \Delta x} \right) = f\left( x \right) + J{\left( x \right)^T}\Delta x + O\left( {\Delta x} \right)
$$
其中：$J(x)^T$ 为 $f(x)$ 关于 $x$ 的导数，为 $n \times 1$ 的列向量。根据上式展开，我们获得最小二乘目标函数：
$$
\begin{array}{l}
  \frac{1}{2}{\left\| {f\left( x \right) + J{{\left( x \right)}^T}} \right\|^2} 
  
  &= \frac{1}{2}{\left( {f\left( x \right) + J{{\left( x \right)}^T}\Delta x} \right)^T}\left( {f\left( x \right) + J{{\left( x \right)}^T}\Delta x} \right) \hfill \\
  & = \frac{1}{2}\left( {f{{\left( x \right)}^T} + \Delta {x^T}J\left( x \right)} \right)\left( {f\left( x \right) + J{{\left( x \right)}^T}\Delta x} \right) \hfill \\
  & = \frac{1}{2}\left( {\left\| {f\left( x \right)} \right\|_2^2 + 2f\left( x \right)J{{\left( x \right)}^T}\Delta x + \Delta {x^T}J\left( x \right)J\left( x \right)^T\Delta x} \right) \hfill \\ 
\end{array}
$$
令上式关于 $\Delta x$ 的导数为 $0$：
$$
J\left( x \right)f\left( x \right) + J\left( x \right){J^T}\left( x \right)\Delta x = 0
$$
改写为下述增量方程：
$$
\underbrace {J\left( x \right){J^T}\left( x \right)}_{H\left( x \right)}\Delta x = \underbrace { - J\left( x \right)f\left( x \right)}_{g\left( x \right)}
$$
记为：
$$
H \Delta x =g
$$
具体迭代步骤如下：

1. 给定初始值：$x_0$
2. 对于第 $k$ 次迭代，求出当前的雅克比矩阵：$J(x_k)$ 和函数值 $f(x)$
3. 求解增量方程：$H \Delta x_k = g$
4. 若 $\Delta x_k$ 足够小，则停止。否则，返回第二步，令：$x_{k+1}=x_k + \Delta x_k$。

在高斯牛顿方法中，$JJ^T$ 替代了牛顿法中的二阶海森矩阵，从而简略了计算量。但为了求解增量方程，需要求解：$H^{-1}$，但实际数据计算出来的 $JJ^T$ 是半正定的，如果其为奇异矩阵或者病态矩阵，增量的求解稳定性很差。另外，就算采用SVD方法来求解方程，$H$ 也并非奇异或者病态，如果我们求解出来的步长 $\Delta x$ 太大，因为是一阶展开，我们的局部近似展开也不够准确，这样也可能走出锯齿状的形状。

## 3.4 L-M方法

二阶泰勒展开只有在站开点附近局部近似才较为准确，因此我们给 $\Delta x$ 添加一个范围，用指标 $\rho $ 来刻画近似模型和实际函数之间的差异，如果近似效果好，那么扩大近似范围，如果近似效果差，则缩小近似范围：
$$
\rho {\text{ = }}\frac{{f\left( {x + \Delta x} \right) - f\left( x \right)}}{{J{{\left( x \right)}^T}\Delta  x}}
$$
公式含义如下：

- 分子：实际函数下降值
- 分母：近似模型下降值

$\rho$ 分为三种情况：

- $\rho $ 接近于1：近似较好
- $\rho$ 小于1：实际减小的值远小于近似减小的值（步子迈地太大），需要减小近似范围
- $\rho$ 大于1：实际下降的值大于近似减小的值（步子迈地太小），需要增大近似范围

---

改良版本的[框架](https://blog.csdn.net/weishaodong/article/details/107212862?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2-107212862-blog-107187743.pc_relevant_antiscanv2&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2-107212862-blog-107187743.pc_relevant_antiscanv2&utm_relevant_index=2)：

1. 给定初始值 $x_0$，初始优化半径 $\mu$，以及阈值 $e$

2. 对于第 $k$ 次迭代，按照公式计算 $\rho$，如果：

   - $\rho > \frac{3}{4}$：设置 $\mu = 2 \mu$
   - $\rho < \frac{1}{4}$：设置 $\mu = 0.5 \mu$
   - $\rho$ 大于阈值 $e$，则认为近似是可行的，令 $x_{k + 1} = x_k + \Delta x_k$

3. 在高斯牛顿基础上添加信赖区间，求解：
   $$
   \mathop {\min }\limits_{\Delta {x_k}} \frac{1}{2}{\left\| {f\left( {{x_k}} \right) + J{{\left( {{x_k}} \right)}^T}\Delta {x_k}} \right\|^2},s.t.{\left\| {D\Delta {x_k}} \right\|^2} \leqslant \mu \label{eq57}
   $$
   其中：

   - $D$：系数矩阵（之后说明）
   - $\mu$：信赖区域半径

4. 判断算法是否收敛（$\Delta x_k$ 小于某一阈值），如不收敛，则返回第二步迭代

对于公式 $\ref{eq57}$，这是一个带不等式的约束优化问题，利用拉格朗日法进行优化，构建拉格朗日函数：
$$
\Gamma \left( {\Delta {x_k},\lambda } \right) = \frac{1}{2}{\left\| {f\left( {{x_k}} \right) + J{{\left( {{x_k}} \right)}^T}\Delta {x_k}} \right\|^2} + \frac{\lambda }{2}\left( {{{\left\| {D\Delta {x_k}} \right\|}^2} - \mu } \right)
$$
其中：$\lambda $ 为拉格朗日乘子。令该函数关于 $\Delta x$ 的导数为零，得到增量方程：
$$
\left( {\underbrace {J\left( x \right){J^T}\left( x \right)}_H + \lambda {D^T}D} \right)\Delta {x_k} = \underbrace { - J\left( x \right)f\left( x \right)}_g
$$
$D$ 有两种方式：

- 列文伯格：$D$ 单位矩阵 $I$，上式即简化为：
  $$
  (H + \lambda I) \Delta x_k =g
  $$

- 马夸尔特：$D$ 为 $J^TJ$ 的对角元素平方根

可以看出：

- 当 $\lambda$ 较小时，$H$ 占主要地位，该模型更接近高斯牛顿法
- 当$\lambda$ 较大时，该方法更接近最速下降法

问题性质较好时候，我们用高斯牛顿，否则就用列文伯格。一般而言，都提供了现成的数学库给我们调用，因此并不用自己手写。

## 3.5 手写高斯牛顿法

考虑一条曲线：
$$
y = exp(a x^2 + bx +c) + w
$$
其中：$a,b,c$ 为曲线的参数，$w$ 为非高斯噪声，满足：$w \sim \left( {0,{\sigma ^2}} \right)$。假设我们现在有 $N$ 个关于 $x,y$ 的观测数据点，那么构建最小二乘损失函数：
$$
F(x) \to \mathop {\min }\limits_{a,b,c} \frac{1}{2}\sum\limits_{i = 1}^N {{{\left\| {{y_i} - \exp \left( {ax_i^2 + b{x_i} + c} \right)} \right\|}^2}}
$$
误差：
$$
f(x) \to e_i = y_i - exp(a x_i^2 + bx_i +c)
$$
误差对于每个参数的偏导数：
$$
\begin{array}{l}
  \frac{{\partial {e_i}}}{{\partial a}} &=  - x_i^2\exp \left( {ax_i^2 + b{x_i} + c} \right) \hfill \\
  \frac{{\partial {e_i}}}{{\partial b}} &=  - {x_i}\exp \left( {ax_i^2 + b{x_i} + c} \right) \hfill \\
  \frac{{\partial {e_i}}}{{\partial c}} &=  - \exp \left( {ax_i^2 + b{x_i} + c} \right) \hfill \\ 
\end{array} 
$$
于是：${J_i} = {\left[ {\frac{{\partial {e_i}}}{{\partial a}},\frac{{\partial {e_i}}}{{\partial b}},\frac{{\partial {e_i}}}{{\partial c}}} \right]^T}$，高斯牛顿的增量方程公式：
$$
\underbrace {J\left( x \right){J^T}\left( x \right)}_{H\left( x \right)}\Delta x = \underbrace { - J\left( x \right)f\left( x \right)}_{g\left( x \right)}
$$
那么代入可得（公式中 $a, b,c$ 才是未知数）：
$$
\left( {\sum\limits_{i = 1}^{100} {{J_i}\left[ {{{\left( {{\sigma ^2}} \right)}^{ - 1}}} \right]J_i^T} } \right)\Delta {x_k} = \sum\limits_{i = 1}^{100} {{J_i}\left[ {{{\left( {{\sigma ^2}} \right)}^{ - 1}}} \right]{e_i}} 
$$

> 在数值计算上，虽然这里的 ${{{\left( {{\sigma ^2}} \right)}^{ - 1}}}$ 可以进行删除（只是因为信息矩阵是一维的），但实际SLAM中，通常状态估计采用的范数是以信息矩阵（即噪声分布的方差）为内积度量矩阵的（而非线性优化推导采用的是普通的二范数）。使用信息矩阵作为内积度量矩阵的高斯牛顿法推导如下：
> $$
> \begin{array}{l}
>   F(x)&=\frac{1}{2}\left\| {f\left( x \right) + {J^T}\left( x \right)\Delta x} \right\|_{\sum {^{ - 1}} }^2 
>  \\ &= \frac{1}{2}{\left( {f\left( x \right) + {J^T}\left( x \right)\Delta x} \right)^T}\sum {^{ - 1}} \left( {f\left( x \right) + {J^T}\left( x \right)\Delta x} \right) \hfill \\
>    &= \frac{1}{2}\left( {\left\| {f\left( x \right)} \right\|_{\sum {^{ - 1}} }^2 + 2J\left( x \right)\Delta x\sum {^{ - 1}} f\left( x \right) + \Delta {x^T}J\left( x \right)\sum {^{ - 1}} {J^T}\left( x \right)\Delta x} \right) \hfill \\ 
> \end{array}
> $$
> 求导令其为零：
> $$
> \begin{gathered}
>   J\left( x \right)\sum {^{ - 1}} f\left( x \right) + J\left( x \right)\sum {^{ - 1}} {J^T}\left( x \right)\Delta x{\text{ = }}0 \hfill \\
>    \downarrow  \hfill \\
>   \underbrace {J\left( x \right)\sum {^{ - 1}} {J^T}\left( x \right)}_{H\left( x \right)}\Delta x{\text{ = }}\underbrace {J\left( x \right)\sum {^{ - 1}} f\left( x \right)}_{g\left( x \right)} \hfill \\ 
> \end{gathered}
> $$

代码：

```c++
#include <iostream>
#include <chrono>
#include <opencv2/opencv.hpp>
#include <Eigen/Core>
#include <Eigen/Dense>

using namespace std;
using namespace Eigen;

//先根据模型生成x，y的真值
//在真值中添加高斯分布的噪声
//使用高斯牛顿法从带噪声的数据拟合参数模型

int main(int argc, char **argv) {
    // 01 生成随机数据
  double ar = 1.0, br = 2.0, cr = 1.0;         // 真实参数值  最后就是要迭代到跟这个真值接近
  double ae = 2.0, be = -1.0, ce = 5.0;        // 估计参数值  随便给定的一个初值，用于迭代到真值附近
  int N = 100;                                 // 数据点
  double w_sigma = 1.0;                        // 噪声Sigma值
  double inv_sigma = 1.0 / w_sigma;
  cv::RNG rng;                                 // OpenCV随机数产生器

  vector<double> x_data, y_data;      // 数据
  for (int i = 0; i < N; i++) {
    double x = i / 100.0;
    x_data.push_back(x);
    y_data.push_back(exp(ar * x * x + br * x + cr) + rng.gaussian(w_sigma * w_sigma));
  }

  // 开始Gauss-Newton迭代
  int iterations = 100;    // 迭代次数
  double cost = 0, lastCost = 0;  // 本次迭代的cost和上一次迭代的cost

  chrono::steady_clock::time_point t1 = chrono::steady_clock::now();
  for (int iter = 0; iter < iterations; iter++)
  {
    Matrix3d H = Matrix3d::Zero();             // Hessian = J^T W^{-1} J in Gauss-Newton
    Vector3d b = Vector3d::Zero();             // bias
    cost = 0;

    for (int i = 0; i < N; i++) {
      double xi = x_data[i], yi = y_data[i];  // 第i个数据点
      double error = yi - exp(ae * xi * xi + be * xi + ce);
      Vector3d J; // 雅可比矩阵
      J[0] = -xi * xi * exp(ae * xi * xi + be * xi + ce);  // de/da
      J[1] = -xi * exp(ae * xi * xi + be * xi + ce);  // de/db
      J[2] = -exp(ae * xi * xi + be * xi + ce);  // de/dc

      H += inv_sigma * inv_sigma * J * J.transpose();
      b += -inv_sigma * inv_sigma * error * J;

      cost += error * error;
    }

    // 求解线性方程 Hx=b（调用Eigen）
    Vector3d dx = H.ldlt().solve(b);
    if (isnan(dx[0])) {
      cout << "result is nan!" << endl;
      break;
    }
      
	// 设置了一个较为简单的终止条件
    if (iter > 0 && cost >= lastCost) {
      cout << "cost: " << cost << ">= last cost: " << lastCost << ", break." << endl;
      break;
    }

    ae += dx[0];
    be += dx[1];
    ce += dx[2];

    lastCost = cost;

    cout << "total cost: " << cost << ", \t\tupdate: " << dx.transpose() <<
         "\t\testimated params: " << ae << "," << be << "," << ce << endl;
  }

  chrono::steady_clock::time_point t2 = chrono::steady_clock::now();
  chrono::duration<double> time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
  cout << "solve time cost = " << time_used.count() << " seconds. " << endl;

  cout << "estimated abc = " << ae << ", " << be << ", " << ce << endl;
  return 0;
}
```



































